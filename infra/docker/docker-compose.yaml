services:
  # PostgreSQL Database - Official PostgreSQL Image
  postgres:
    container_name: niki-postgres
    image: postgres:16-alpine
    platform: linux/amd64 # Explicit platform for M4 compatibility
    env_file:
      - ../../.env
    environment:
      POSTGRES_PASSWORD: ${POSTGRESQL_PASSWORD}
      POSTGRES_USER: ${POSTGRESQL_USERNAME}
      POSTGRES_DB: ${POSTGRESQL_DATABASE}
      PGTZ: ${POSTGRESQL_TIMEZONE}
      # PostgreSQL performance tuning via command arguments
    command:
      - 'postgres'
      - '-c'
      - 'max_connections=${POSTGRESQL_MAX_CONNECTIONS}'
      - '-c'
      - 'shared_buffers=${POSTGRESQL_SHARED_BUFFERS}'
      - '-c'
      - 'effective_cache_size=${POSTGRESQL_EFFECTIVE_CACHE_SIZE}'
      - '-c'
      - 'work_mem=${POSTGRESQL_WORK_MEM}'
      - '-c'
      - 'maintenance_work_mem=${POSTGRESQL_MAINTENANCE_WORK_MEM}'
      - '-c'
      - 'timezone=${POSTGRESQL_TIMEZONE}'
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init-databases.sql:/docker-entrypoint-initdb.d/init-databases.sql:ro
    ports:
      - '${POSTGRESQL_PORT}:5432'
    restart: unless-stopped
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRESQL_USERNAME} -d ${POSTGRESQL_DATABASE}']
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512MB
        reservations:
          cpus: '0.2'
          memory: 256MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Zookeeper - Optimized for Apple Silicon M4
  zookeeper:
    container_name: niki-zookeeper
    image: confluentinc/cp-zookeeper:7.6.1
    platform: linux/amd64
    env_file:
      - ../../.env
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
      ZOOKEEPER_TICK_TIME: ${ZOOKEEPER_TICK_TIME}
      ZOOKEEPER_INIT_LIMIT: ${ZOOKEEPER_INIT_LIMIT}
      ZOOKEEPER_SYNC_LIMIT: ${ZOOKEEPER_SYNC_LIMIT}
    ports:
      - '${ZOOKEEPER_PORT}:2181'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 256MB
        reservations:
          cpus: '0.1'
          memory: 128MB
    volumes:
      - zookeeper_data:/tmp/zookeeper-logs
    healthcheck:
      test: ['CMD-SHELL', 'echo ruok | nc localhost 2181 || exit 1']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Kafka - Optimized for Apple Silicon M4
  kafka:
    container_name: niki-kafka
    image: confluentinc/cp-kafka:7.6.1
    platform: linux/amd64
    depends_on:
      zookeeper:
        condition: service_healthy
    env_file:
      - ../../.env
    ports:
      - '${KAFKA_INTERNAL_PORT}:9092'
      - '${KAFKA_EXTERNAL_PORT}:9094'
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: ${KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS}
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: ${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: ${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_INTER_BROKER_LISTENER_NAME: ${KAFKA_INTER_BROKER_LISTENER_NAME}
      KAFKA_LISTENERS: ${KAFKA_LISTENERS}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: ${KAFKA_AUTO_CREATE_TOPICS_ENABLE}
      KAFKA_DELETE_TOPIC_ENABLE: ${KAFKA_DELETE_TOPIC_ENABLE}
      KAFKA_LOG_RETENTION_HOURS: ${KAFKA_LOG_RETENTION_HOURS}
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS}
      KAFKA_DEFAULT_REPLICATION_FACTOR: ${KAFKA_DEFAULT_REPLICATION_FACTOR}
      KAFKA_MIN_INSYNC_REPLICAS: ${KAFKA_MIN_INSYNC_REPLICAS}
    restart: unless-stopped
    volumes:
      - kafka_data:/tmp/kafka-logs
    healthcheck:
      test: ['CMD-SHELL', 'kafka-topics --bootstrap-server localhost:9092 --list || exit 1']
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.8'
          memory: 1GB
        reservations:
          cpus: '0.3'
          memory: 512MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Schema Registry - Schema management for Kafka
  schema-registry:
    container_name: niki-schema-registry
    image: confluentinc/cp-schema-registry:7.6.1
    platform: linux/amd64
    hostname: ${SCHEMA_REGISTRY_HOST_NAME}
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy
    ports:
      - '${SCHEMA_REGISTRY_PORT}:8081'
    environment:
      # Schema Registry Configuration - Using explicit variables only (no env_file to avoid conflicts)
      SCHEMA_REGISTRY_HOST_NAME: ${SCHEMA_REGISTRY_HOST_NAME}
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: ${SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS}
      SCHEMA_REGISTRY_LISTENERS: ${SCHEMA_REGISTRY_LISTENERS}
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: ${SCHEMA_REGISTRY_KAFKASTORE_TOPIC}
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: ${SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR}
      SCHEMA_REGISTRY_DEBUG: ${SCHEMA_REGISTRY_DEBUG}
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: ${SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL}
      SCHEMA_REGISTRY_HEAP_OPTS: ${SCHEMA_REGISTRY_HEAP_OPTS}
      SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL: ${SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL}
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: ${SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL}
      SCHEMA_REGISTRY_SCHEMA_REGISTRY_GROUP_ID: ${SCHEMA_REGISTRY_SCHEMA_REGISTRY_GROUP_ID}
      SCHEMA_REGISTRY_MASTER_ELIGIBILITY: ${SCHEMA_REGISTRY_MASTER_ELIGIBILITY}
      SCHEMA_REGISTRY_KAFKASTORE_TIMEOUT_MS: ${SCHEMA_REGISTRY_KAFKASTORE_TIMEOUT_MS}
    restart: unless-stopped
    volumes:
      - kafka_schema_registry_data:/var/lib/schema-registry
      - kafka_schema_registry_secrets:/etc/schema-registry/secrets
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:8081/subjects || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512MB
        reservations:
          cpus: '0.2'
          memory: 256MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'
    networks:
      - default

  # Control Center - Optional monitoring interface
  control-center:
    container_name: niki-control-center
    image: confluentinc/cp-enterprise-control-center:7.6.1
    platform: linux/amd64
    hostname: control-center
    depends_on:
      kafka:
        condition: service_healthy
    env_file:
      - ../../.env
    ports:
      - '${CONTROL_CENTER_PORT}:9021'
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: ${CONTROL_CENTER_BOOTSTRAP_SERVERS}
      CONTROL_CENTER_REPLICATION_FACTOR: ${CONTROL_CENTER_REPLICATION_FACTOR}
      CONTROL_CENTER_CONNECT_CLUSTER: ${CONTROL_CENTER_CONNECT_CLUSTER}
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: ${CONTROL_CENTER_SCHEMA_REGISTRY_URL}
      PORT: ${CONTROL_CENTER_PORT}
      CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS: ${CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS}
      CONTROL_CENTER_STREAMS_CACHE_MAX_BYTES_BUFFERING: ${CONTROL_CENTER_STREAMS_CACHE_MAX_BYTES_BUFFERING}
      CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: ${CONTROL_CENTER_CONNECT_CONNECT_DEFAULT_CLUSTER}
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: ${CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS}
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: ${CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS}
      CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT: ${CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT}
      CONFLUENT_METRICS_TOPIC_REPLICATION: ${CONFLUENT_METRICS_TOPIC_REPLICATION}
    restart: unless-stopped
    volumes:
      - kafka_control_center_data:/tmp/control-center-logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1GB
        reservations:
          cpus: '0.2'
          memory: 512MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Kafka Connect - Stream processing and connectors
  kafka-connect:
    container_name: niki-kafka-connect
    image: confluentinc/cp-kafka-connect-base:7.5.10
    platform: linux/amd64
    hostname: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
    env_file:
      - ../../.env
    ports:
      - '${KAFKA_CONNECT_PORT}:8083'
    environment:
      CONNECT_BOOTSTRAP_SERVERS: ${CONNECT_BOOTSTRAP_SERVERS}
      CONNECT_REST_PORT: ${CONNECT_REST_PORT}
      CONNECT_GROUP_ID: ${CONNECT_GROUP_ID}
      CONNECT_CONFIG_STORAGE_TOPIC: ${CONNECT_CONFIG_STORAGE_TOPIC}
      CONNECT_OFFSET_STORAGE_TOPIC: ${CONNECT_OFFSET_STORAGE_TOPIC}
      CONNECT_STATUS_STORAGE_TOPIC: ${CONNECT_STATUS_STORAGE_TOPIC}
      CONNECT_KEY_CONVERTER: ${CONNECT_KEY_CONVERTER}
      CONNECT_VALUE_CONVERTER: ${CONNECT_VALUE_CONVERTER}
      CONNECT_INTERNAL_KEY_CONVERTER: ${CONNECT_INTERNAL_KEY_CONVERTER}
      CONNECT_INTERNAL_VALUE_CONVERTER: ${CONNECT_INTERNAL_VALUE_CONVERTER}
      CONNECT_REST_ADVERTISED_HOST_NAME: ${CONNECT_REST_ADVERTISED_HOST_NAME}
      CONNECT_LOG4J_ROOT_LOGLEVEL: ${CONNECT_LOG4J_ROOT_LOGLEVEL}
      CONNECT_LOG4J_LOGGERS: ${CONNECT_LOG4J_LOGGERS}
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: ${CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN}
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: ${CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR}
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: ${CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR}
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: ${CONNECT_STATUS_STORAGE_REPLICATION_FACTOR}
      CONNECT_PLUGIN_PATH: ${CONNECT_PLUGIN_PATH}
    command:
      - bash
      - -c
      - |
        echo "Installing Connector"
        confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:14.1.5
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
    volumes:
      - kafka_connect_data:/tmp/connect-logs
    restart: unless-stopped
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:8083/connectors || exit 1']
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.8'
          memory: 1GB
        reservations:
          cpus: '0.3'
          memory: 512MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Elasticsearch - Search and analytics engine
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0
    container_name: niki-elasticsearch
    platform: linux/arm64
    env_file:
      - ../../.env
    environment:
      - discovery.type=${ELASTICSEARCH_DISCOVERY_TYPE}
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME}
      - node.name=${ELASTICSEARCH_NODE_NAME}
      - bootstrap.memory_lock=${ELASTICSEARCH_BOOTSTRAP_MEMORY_LOCK}
      - xpack.security.enabled=${ELASTICSEARCH_XPACK_SECURITY_ENABLED}
      - xpack.security.enrollment.enabled=${ELASTICSEARCH_XPACK_SECURITY_ENROLLMENT_ENABLED}
      - xpack.monitoring.collection.enabled=${ELASTICSEARCH_XPACK_MONITORING_COLLECTION_ENABLED}
      - indices.queries.cache.size=${ELASTICSEARCH_INDICES_QUERIES_CACHE_SIZE}
      - indices.fielddata.cache.size=${ELASTICSEARCH_INDICES_FIELDDATA_CACHE_SIZE}
      - thread_pool.write.queue_size=${ELASTICSEARCH_THREAD_POOL_WRITE_QUEUE_SIZE}
      - http.cors.enabled=${ELASTICSEARCH_HTTP_CORS_ENABLED}
      - action.destructive_requires_name=${ELASTICSEARCH_ACTION_DESTRUCTIVE_REQUIRES_NAME}
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - '${ELASTICSEARCH_PORT}:9200'
      - '9300:9300'
    restart: unless-stopped
    healthcheck:
      test:
        ['CMD-SHELL', 'curl -s -f http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GB
        reservations:
          cpus: '0.5'
          memory: 1GB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Kibana - Visualization for Elasticsearch
  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.0
    container_name: niki-kibana
    platform: linux/arm64
    depends_on:
      elasticsearch:
        condition: service_healthy
    env_file:
      - ../../.env
    ports:
      - '${KIBANA_PORT}:5601'
    environment:
      - ELASTICSEARCH_HOSTS=${KIBANA_ELASTICSEARCH_HOSTS}
      - NODE_OPTIONS=${KIBANA_NODE_OPTIONS}
    volumes:
      - kibana_data:/usr/share/kibana/data
    restart: unless-stopped
    healthcheck:
      test: ['CMD-SHELL', 'curl -s -f http://localhost:5601/api/status || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512MB
        reservations:
          cpus: '0.2'
          memory: 256MB
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # Users Server - User authentication and management
  # users-server:
  #   container_name: niki-users-server
  #   build:
  #     context: ../../ # Contexto na RAIZ do projeto (relativo a infra/docker)
  #     dockerfile: ./apps/users-server/Dockerfile
  #     target: production # Use the production stage
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     kafka:
  #       condition: service_healthy
  #     kafka-connect:
  #       condition: service_healthy
  #   env_file:
  #     - ../../.env
  #   ports:
  #     - '${USERS_SERVER_PORT}:${USERS_SERVER_PORT}'
  #   environment:
  #     - ENVIRONMENT=${ENVIRONMENT}
  #     # Message Broker
  #     - MESSAGE_BROKER_PROVIDER_BROKER_URL=${MESSAGE_BROKER_PROVIDER_BROKER_URL}
  #       # JWT
  #     - TOKEN_PROVIDER_JWT_ISSUER=${TOKEN_PROVIDER_JWT_ISSUER}
  #     - TOKEN_PROVIDER_JWT_SECRET=${TOKEN_PROVIDER_JWT_SECRET}
  #     - TOKEN_PROVIDER_JWT_EXPIRES_IN_MINUTES=${TOKEN_PROVIDER_JWT_EXPIRES_IN_MINUTES}
  #     - TOKEN_PROVIDER_JWT_ALGORITHM=${TOKEN_PROVIDER_JWT_ALGORITHM}
  #     - USERS_SERVER_PORT=${USERS_SERVER_PORT}
  #     - USERS_SERVER_DATABASE_URL=${USERS_SERVER_DATABASE_URL}
  #   volumes:
  #     - users_server_data:/app/data
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ['CMD-SHELL', 'curl -f http://localhost:${USERS_SERVER_PORT}/healthcheck || exit 1']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 40s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 512MB
  #       reservations:
  #         cpus: '0.2'
  #         memory: 256MB
  #   logging:
  #     driver: 'json-file'
  #     options:
  #       max-size: '10m'
  #       max-file: '3'

volumes:
  users_server_data:
    driver: local
    name: niki-users-server-data

  postgres_data:
    driver: local
    name: niki-postgres-data

  elasticsearch_data:
    driver: local
    name: niki-elasticsearch-data

  connect_data:
    driver: local
    name: niki-connect-data

  kibana_data:
    driver: local
    name: niki-kibana-data

  kafka_control_center_data:
    driver: local
    name: niki-kafka-control-center-data

  kafka_schema_registry_data:
    driver: local
    name: niki-kafka-schema-registry-data

  kafka_schema_registry_secrets:
    driver: local
    name: niki-kafka-schema-registry-secrets

  kafka_connect_data:
    driver: local
    name: niki-kafka-connect-data

  kafka_data:
    driver: local
    name: niki-kafka-data

  zookeeper_data:
    driver: local
    name: niki-zookeeper-data
